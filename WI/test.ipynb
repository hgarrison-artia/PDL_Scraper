{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted tables from page 2 and saved to page_2_tables.csv\n"
     ]
    }
   ],
   "source": [
    "import tabula\n",
    "import os\n",
    "\n",
    "pdf_path = 'WI.pdf'\n",
    "output_csv_path = 'page_2_tables.csv'\n",
    "page_number = 2\n",
    "\n",
    "tabula.convert_into(\n",
    "    pdf_path,\n",
    "    output_csv_path,\n",
    "    output_format='csv',\n",
    "    pages=str(page_number),\n",
    "    lattice=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content:\n",
      "Wisconsin Medicaid, BadgerCare Plus Standard, and SeniorCare Preferred Drug List – Quick Reference\n",
      "Revised 05/29/2025 Effective 05/01/2025\n",
      "Acne Agents, Topical Analgesics/Anesthetics, Topical (cont) Analgesics, Opioids Long-Acting (cont) Androgenic Agents (cont)\n",
      "adapalene 0.1% cream P lidocaine 5% ointment P oxycodone ER NP Depo-testosterone* P\n",
      "adapalene OTC 0.1% gel P lidocaine 5% trans patch P oxymorphone ER NP methyltestosterone capsule NP\n",
      "adapalene 0.3% gel P diclofenac 1.3% patch tramadol ER cap (Gen-Conzip) SCN NP testosterone gel packet\n",
      "NP NP\n",
      "benzoyl peroxide OTC 2.5%, (Gen-Flector) tramadol ER tab (Gen-Ryzolt) NP (Gen-Androgel)\n",
      "SCN P\n",
      "5%, 10% diclofenac 1.5% solution Belbuca Film NP testosterone pump\n",
      "NP NP\n",
      "clindamycin/benzoyl peroxide (Gen-Pennsaid solution) Conzip SCN NP (Gen-Axiron and Fortesta)\n",
      "P\n",
      "(Gen-Duac) diclofenac 2% pump Oxycontin NP Azmiro* NP\n",
      "NP\n",
      "clindamycin gel (Gen-Cleocin T) P (Gen-Pennsaid pump) Analgesics, Opioids Short-Acting Jatenzo SCN NP\n",
      "clindamycin solution P Flector NP codeine/apap P Methitest tablet NP\n",
      "erythromycin gel, solution P Licart patch SCN NP hydromorphone P Natesto nasal spray SCN NP\n",
      "sodium sulfacetamide-sulfur Pennsaid pump SCN NP hydrocodone/apap 325mg P Tlando capsule NP\n",
      "SCN P\n",
      "10-5% cleanser Ztlido SCN NP hydrocodone/ibuprofen P Testim SCN NP\n",
      "sulfacetamide sodium susp P Analgesics, Miscellaneous morphine P Undecatrex capsule SCN NP\n",
      "Retin-A (not micro) P acetaminophen SCN P oxycodone solution, tablets P Vogelxo NP\n",
      "NOTE: Topical federal legend acne drugs apap chew tab 80mg, 160mg* P oxycodone/apap 325mg P Xyosted NP\n",
      "not listed are either non-preferred or NP aspirin SCN P tramadol 50mg tab P * Policy for obtaining provider-administered drugs\n",
      "noncovered. ibuprofen Rx P tramadol/apap 325mg P applies. Refer to topic #5697.\n",
      "Alzheimer’s Agents ibuprofen OTC SCN P butorphanol spray NP Angiotensin Modulators, ACE Inhibitors\n",
      "donepezil 5mg,10mg P ibuprofen OTC chew tab 100mg* P codeine NP benazepril P\n",
      "donepezil ODT 5mg, naproxen Rx P fentanyl citrate oral transmucosal captopril P\n",
      "P NP\n",
      "10mg naproxen OTC SCN P lozenges enalapril solution\n",
      "memantine solution, butalbital/apap NP levorphanol NP (Gen-Epaned) SCN P\n",
      "P\n",
      "tablet, titration pack* butalbital/apap/caffeine NP hydrocodone/apap* NP enalapril tablet P\n",
      "rivastigmine caps P butalbital/apap/caffeine/codeine NP hydromorphone liquid, suppository NP enalapril/HCTZ P\n",
      "Exelon patch P butalbital/asa/caffeine NP meperidine NP fosinopril P\n",
      "donepezil 23mg NP butalbital/asa/caffeine/codeine NP oxycodone capsules, concentrate NP lisinopril P\n",
      "galantamine tablets NP Journavx tablets NP oxymorphone NP lisinopril/HCTZ P\n",
      "galantamine ER caps NP * Products are only covered for members 12 pentazocine/naloxone NP quinapril P\n",
      "galantamine solution NP years of age or younger tramadol HCL solution NP ramipril P\n",
      "memantine/donepezil ER NP Analgesics, Opioids Long-Acting tramadol 25mg, 75mg tablet SCN NP benazepril/HCTZ NP\n",
      "caps (Gen-Namzaric) fentanyl transdermal 12mcg, tramadol 100mg tablet NP captopril/HCTZ SCN NP\n",
      "memantine ER caps DR NP 25mcg, 50mcg, 75mcg, 100mcg P Dilaudid Liquid NP fosinopril/HCTZ NP\n",
      "(Gen-Namenda XR)* morphine ER tablets P Fentora NP moexipril NP\n",
      "rivastigmine patch NP tramadol ER tab (Gen-Ultram ER) P Roxybond Tablet SCN NP perindopril NP\n",
      "Adlarity patch SCN NP Butrans transdermal P Seglentis tablet SCN NP quinapril/HCTZ NP\n",
      "Namzaric capsule NP Hysingla ER P *Combination products containing any other trandolapril NP\n",
      "Namzaric dose pack NP buprenorphine transdermal NP strength of apap besides 325 mg. Qbrelis solution SCN NP\n",
      "Zunveyl tablets SCN NP fentanyl transdermal 37.5mcg, NP Androgenic Agents Angiotensin Modulators, ARBs and DRIs\n",
      "*memantine products are not covered for 62.5mcg, 87.5mcg oxandrolone tablet P irbesartan P\n",
      "members 17 years of age or younger hydrocodone ER tablet NP testosterone cypionate* P irbesartan/HCTZ P\n",
      "Analgesics/Anesthetics, Topical (Gen-Hysingla ER) testosterone enanthate* P losartan P\n",
      "c da icp ls oa feic ni an c O 1T %C gel SCN P h (Gyd er no -c Zo od ho yn de ro E ER R c ) apsule NP t (Ges et no -s Ate nr do rn oe g eg le ) l pump P l oo ls ma er sta an rt/ aH nC TZ P P\n",
      "P\n",
      "(Gen-Voltaren RX) hydromorphone ER NP testosterone gel, pump olmesartan/HCTZ P\n",
      "diclofenac sodium 1% gel OTC methadone tablet, solution NP (Gen-Vogelxo) P valsartan P\n",
      "P\n",
      "(Gen-Voltaren OTC) morphine ER capsules NP Androgel gel pump P valsartan/HCTZ P\n",
      "Page 2 of 13\n",
      "ExU es me ps t iP oA n / FP oD rL m - U Fs oe rms P /SA e/ cD . G VA I GenB er ra icn d (B B Be Gfo )r De rug Use Fs o s rmpe -c aifi vc a D ilaru bg le P A Use Fs o s rmpe -c aif vic a D ilaru blg e P A U Fs oe rms /P SA e/ cD . G VA II Monthly Changes\n",
      "available via STAT-PA Paper PA process only Refer to topic #20077 via STAT-PA or via Paper PA Paper PA process only to the PDL\n",
      "or Paper PA process Refer to topic #15937 Paper PA process process only Refer to topic #15937\n",
      "\n",
      "Word positions:\n",
      "Text: Wisconsin, x0: 52.08, top: 22.319040000000086\n",
      "Text: Medicaid,, x0: 126.00340800000001, top: 22.319040000000086\n",
      "Text: BadgerCare, x0: 193.68463200000002, top: 22.319040000000086\n",
      "Text: Plus, x0: 276.971316, top: 22.319040000000086\n",
      "Text: Standard,, x0: 310.45109999999994, top: 22.319040000000086\n",
      "Text: and, x0: 378.9775319999998, top: 22.319040000000086\n",
      "Text: SeniorCare, x0: 407.65844399999975, top: 22.319040000000086\n",
      "Text: Preferred, x0: 486.3891479999995, top: 22.319040000000086\n",
      "Text: Drug, x0: 552.3953999999997, top: 22.319040000000086\n",
      "Text: List, x0: 589.0018919999998, top: 22.319040000000086\n",
      "\n",
      "Detected table boundaries:\n",
      "Table bounds: (22.459459459459474, 59.27999999999997, 775.1493577981662, 513.9784615384616)\n",
      "Table bounds: (60.12016666666668, 531.2685, 143.39, 585.2000666666665)\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "pdf_path = 'WI.pdf'\n",
    "page_number = 2\n",
    "\n",
    "# Open the PDF and analyze page 2\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    page = pdf.pages[page_number-1]\n",
    "    \n",
    "    # Extract text to understand the structure\n",
    "    text = page.extract_text()\n",
    "    print(\"Text content:\")\n",
    "    print(text)\n",
    "    \n",
    "    # Extract words with their positions\n",
    "    words = page.extract_words()\n",
    "    print(\"\\nWord positions:\")\n",
    "    for word in words[:10]:  # Print first 10 words as example\n",
    "        print(f\"Text: {word['text']}, x0: {word['x0']}, top: {word['top']}\")\n",
    "    \n",
    "    # Try to detect table boundaries\n",
    "    tables = page.find_tables()\n",
    "    print(\"\\nDetected table boundaries:\")\n",
    "    for table in tables:\n",
    "        print(f\"Table bounds: {table.bbox}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of extracted data:\n",
      "                            0                             1  \\\n",
      "0               (Gen-Flector)  tramadol ER tab (Gen-Ryzolt)   \n",
      "1  benzoyl peroxide OTC 2.5%,                          None   \n",
      "2                     SCN  P                        5%, 10%   \n",
      "3                   NP    NP        (Gen-Pennsaid solution)   \n",
      "4                   NP    NP   clindamycin/benzoyl peroxide   \n",
      "\n",
      "                           2               3     4                  5     6  \\\n",
      "0                         NP  (Gen-Androgel)  None               None  None   \n",
      "1                       None            None  None               None  None   \n",
      "2   diclofenac 1.5% solution    Belbuca Film    NP  testosterone pump  None   \n",
      "3  (Gen-Axiron and Fortesta)            None  None               None  None   \n",
      "4                     Conzip             SCN    NP               None  None   \n",
      "\n",
      "      7  \n",
      "0  None  \n",
      "1  None  \n",
      "2  None  \n",
      "3  None  \n",
      "4  None  \n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, page_number=2):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        page = pdf.pages[page_number-1]\n",
    "        \n",
    "        # Extract all words with their positions\n",
    "        words = page.extract_words(\n",
    "            x_tolerance=3,\n",
    "            y_tolerance=3,\n",
    "            keep_blank_chars=True,\n",
    "            use_text_flow=True,\n",
    "            horizontal_ltr=True\n",
    "        )\n",
    "        \n",
    "        # Group words by their vertical position (y-coordinate)\n",
    "        # This helps identify rows\n",
    "        rows = {}\n",
    "        for word in words:\n",
    "            # Round the y-coordinate to handle slight misalignments\n",
    "            y_pos = round(word['top'])\n",
    "            if y_pos not in rows:\n",
    "                rows[y_pos] = []\n",
    "            rows[y_pos].append(word)\n",
    "        \n",
    "        # Sort rows by y-position\n",
    "        sorted_rows = sorted(rows.items())\n",
    "        \n",
    "        # Process each row\n",
    "        data = []\n",
    "        current_category = None\n",
    "        \n",
    "        for y_pos, row_words in sorted_rows:\n",
    "            # Skip header rows\n",
    "            if y_pos < 100:  # Adjust this threshold based on your PDF\n",
    "                continue\n",
    "                \n",
    "            # Sort words in the row by x-position\n",
    "            row_words.sort(key=lambda x: x['x0'])\n",
    "            \n",
    "            # Extract the text from the row\n",
    "            row_text = ' '.join(word['text'] for word in row_words)\n",
    "            \n",
    "            # Skip empty rows\n",
    "            if not row_text.strip():\n",
    "                continue\n",
    "                \n",
    "            # Check if this is a category row (all caps)\n",
    "            if row_text.isupper() and len(row_text) > 3:\n",
    "                current_category = row_text\n",
    "                continue\n",
    "                \n",
    "            # Split the row into columns based on spacing\n",
    "            # This is a simplified approach - you might need to adjust the splitting logic\n",
    "            columns = re.split(r'\\s{2,}', row_text)\n",
    "            \n",
    "            # Clean up the columns\n",
    "            columns = [col.strip() for col in columns if col.strip()]\n",
    "            \n",
    "            if columns:\n",
    "                # Add the category if we have one\n",
    "                if current_category:\n",
    "                    columns.insert(0, current_category)\n",
    "                data.append(columns)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Clean up the DataFrame\n",
    "        # Remove rows that are just headers or footers\n",
    "        df = df[~df[0].str.contains('Page|Uses PA/PDL|Exemption Form', na=False)]\n",
    "        \n",
    "        # Forward fill category values\n",
    "        df[0] = df[0].fillna(method='ffill')\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Extract tables\n",
    "pdf_path = 'WI.pdf'\n",
    "df = extract_tables_from_pdf(pdf_path)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = 'page_2_tables.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"First few rows of extracted data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page dimensions: 792.0 x 612.0\n",
      "\n",
      "First 500 characters of text:\n",
      "Wisconsin Medicaid, BadgerCare Plus Standard, and SeniorCare Preferred Drug List – Quick Reference\n",
      "Revised 05/29/2025 Effective 05/01/2025\n",
      "Acne Agents, Topical Analgesics/Anesthetics, Topical (cont) Analgesics, Opioids Long-Acting (cont) Androgenic Agents (cont)\n",
      "adapalene 0.1% cream P lidocaine 5% ointment P oxycodone ER NP Depo-testosterone* P\n",
      "adapalene OTC 0.1% gel P lidocaine 5% trans patch P oxymorphone ER NP methyltestosterone capsule NP\n",
      "adapalene 0.3% gel P diclofenac 1.3% patch tramadol E\n",
      "\n",
      "First 20 words with positions:\n",
      "Text: Wisconsin Medicaid, BadgerCare Plus Standard, and SeniorCare Preferred Drug List – Quick Reference  x0: 52.08    x1: 743.83   top: 22.32   \n",
      "Text: Revised 05/29/2025 Effective 05/01/2025  x0: 292.32   x1: 502.78   top: 37.51   \n",
      "Text:                                x0: 396.00   x1: 398.25   top: 49.80   \n",
      "Text: Page 2 of 13                   x0: 702.60   x1: 768.34   top: 519.79  \n",
      "Text:                                x0: 145.56   x1: 154.65   top: 579.71  \n",
      "Text:                                x0: 240.84   x1: 249.93   top: 579.71  \n",
      "Text:                                x0: 336.96   x1: 346.05   top: 579.71  \n",
      "Text:                                x0: 437.52   x1: 446.61   top: 579.71  \n",
      "Text:                                x0: 535.79   x1: 544.89   top: 579.71  \n",
      "Text:                                x0: 634.91   x1: 644.01   top: 579.71  \n",
      "Text:                                x0: 730.19   x1: 732.44   top: 579.71  \n",
      "Text: Monthly Changes                x0: 651.72   x1: 724.30   top: 545.92  \n",
      "Text: to the PDL                     x0: 665.53   x1: 710.51   top: 556.25  \n",
      "Text: Uses PA/DGA                    x0: 564.36   x1: 619.51   top: 540.45  \n",
      "Text: Form/Sec. VII                  x0: 565.32   x1: 616.52   top: 549.69  \n",
      "Text: Paper PA process only          x0: 549.12   x1: 632.72   top: 558.81  \n",
      "Text: Refer to topic #15937          x0: 551.52   x1: 630.33   top: 568.05  \n",
      "Text: Uses specific Drug PA          x0: 448.56   x1: 535.53   top: 540.49  \n",
      "Text: Form - available               x0: 460.32   x1: 526.05   top: 550.33  \n",
      "Text: via Paper PA                   x0: 465.96   x1: 520.29   top: 560.05  \n",
      "\n",
      "Potential column boundaries (x positions):\n",
      "x: 27.84\n",
      "x: 52.08\n",
      "x: 61.68\n",
      "x: 64.80\n",
      "x: 69.96\n",
      "x: 77.28\n",
      "x: 134.52\n",
      "x: 140.28\n",
      "x: 145.56\n",
      "x: 156.84\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_pdf_structure(pdf_path, page_number=2):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        page = pdf.pages[page_number-1]\n",
    "        \n",
    "        # Get the page dimensions\n",
    "        print(f\"Page dimensions: {page.width} x {page.height}\")\n",
    "        \n",
    "        # Extract text with exact positioning\n",
    "        text = page.extract_text()\n",
    "        print(\"\\nFirst 500 characters of text:\")\n",
    "        print(text[:500])\n",
    "        \n",
    "        # Extract words with their exact positions\n",
    "        words = page.extract_words(\n",
    "            x_tolerance=3,\n",
    "            y_tolerance=3,\n",
    "            keep_blank_chars=True,\n",
    "            use_text_flow=True,\n",
    "            horizontal_ltr=True\n",
    "        )\n",
    "        \n",
    "        # Print first 20 words with their positions\n",
    "        print(\"\\nFirst 20 words with positions:\")\n",
    "        for word in words[:20]:\n",
    "            print(f\"Text: {word['text']:<30} x0: {word['x0']:<8.2f} x1: {word['x1']:<8.2f} top: {word['top']:<8.2f}\")\n",
    "        \n",
    "        # Try to identify column boundaries\n",
    "        x_positions = sorted(set(word['x0'] for word in words))\n",
    "        print(\"\\nPotential column boundaries (x positions):\")\n",
    "        for x in x_positions[:10]:  # Print first 10 positions\n",
    "            print(f\"x: {x:.2f}\")\n",
    "\n",
    "# Run the analysis\n",
    "pdf_path = 'WI.pdf'\n",
    "analyze_pdf_structure(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Extract lines\u001b[39;00m\n\u001b[1;32m     75\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWI.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 76\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_lines_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[1;32m     79\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_2_tables.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[25], line 45\u001b[0m, in \u001b[0;36mextract_lines_from_pdf\u001b[0;34m(pdf_path, page_number)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSCN\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     44\u001b[0m     current_text\u001b[38;5;241m.\u001b[39mappend(part)\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_col\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(current_text)\n\u001b[1;32m     46\u001b[0m     current_col \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     47\u001b[0m     current_text \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "def extract_lines_from_pdf(pdf_path, page_number=2):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        page = pdf.pages[page_number-1]\n",
    "        \n",
    "        # Extract text line by line\n",
    "        text = page.extract_text()\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        # Process each line\n",
    "        data = []\n",
    "        current_category = None\n",
    "        \n",
    "        for line in lines:\n",
    "            # Skip header and footer lines\n",
    "            if 'Page' in line or 'Uses PA/PDL' in line or 'Exemption Form' in line:\n",
    "                continue\n",
    "                \n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # Check if this is a category line (all caps)\n",
    "            if line.isupper() and len(line) > 3:\n",
    "                current_category = line\n",
    "                continue\n",
    "            \n",
    "            # Split the line into columns (approximately)\n",
    "            # The line should have 4 columns separated by spaces\n",
    "            parts = line.split()\n",
    "            \n",
    "            # Initialize columns\n",
    "            columns = [''] * 4\n",
    "            \n",
    "            # Try to distribute the parts into columns\n",
    "            current_col = 0\n",
    "            current_text = []\n",
    "            \n",
    "            for part in parts:\n",
    "                # If we see a P or NP, it's likely the end of a column\n",
    "                if part in ['P', 'NP', 'SCN']:\n",
    "                    current_text.append(part)\n",
    "                    columns[current_col] = ' '.join(current_text)\n",
    "                    current_col += 1\n",
    "                    current_text = []\n",
    "                else:\n",
    "                    current_text.append(part)\n",
    "            \n",
    "            # Add any remaining text to the last column\n",
    "            if current_text:\n",
    "                columns[current_col] = ' '.join(current_text)\n",
    "            \n",
    "            # Add the category if we have one\n",
    "            if current_category:\n",
    "                columns.insert(0, current_category)\n",
    "            \n",
    "            # Add the row to our data\n",
    "            data.append(columns)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Clean up the DataFrame\n",
    "        # Remove rows where all columns are empty\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        # Forward fill category values\n",
    "        df[0] = df[0].fillna(method='ffill')\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Extract lines\n",
    "pdf_path = 'WI.pdf'\n",
    "df = extract_lines_from_pdf(pdf_path)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = 'page_2_tables.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"First few rows of extracted data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[25], line 76\u001b[0m\n",
      "\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Extract lines\u001b[39;00m\n",
      "\u001b[1;32m     75\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWI.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m---> 76\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_lines_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n",
      "\u001b[1;32m     79\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_2_tables.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "Cell \u001b[0;32mIn[25], line 45\u001b[0m, in \u001b[0;36mextract_lines_from_pdf\u001b[0;34m(pdf_path, page_number)\u001b[0m\n",
      "\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSCN\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[1;32m     44\u001b[0m     current_text\u001b[38;5;241m.\u001b[39mappend(part)\n",
      "\u001b[0;32m---> 45\u001b[0m     \u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_col\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(current_text)\n",
      "\u001b[1;32m     46\u001b[0m     current_col \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;32m     47\u001b[0m     current_text \u001b[38;5;241m=\u001b[39m []\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "def extract_lines_from_pdf(pdf_path, page_number=2):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        page = pdf.pages[page_number-1]\n",
    "        \n",
    "        # Extract text line by line\n",
    "        text = page.extract_text()\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        # Process each line\n",
    "        data = []\n",
    "        current_category = None\n",
    "        \n",
    "        for line in lines:\n",
    "            # Skip header and footer lines\n",
    "            if 'Page' in line or 'Uses PA/PDL' in line or 'Exemption Form' in line:\n",
    "                continue\n",
    "                \n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # Check if this is a category line (all caps)\n",
    "            if line.isupper() and len(line) > 3:\n",
    "                current_category = line\n",
    "                continue\n",
    "            \n",
    "            # Split the line into columns (approximately)\n",
    "            # The line should have 4 columns separated by spaces\n",
    "            parts = line.split()\n",
    "            \n",
    "            # Initialize columns\n",
    "            columns = [''] * 4\n",
    "            \n",
    "            # Try to distribute the parts into columns\n",
    "            current_col = 0\n",
    "            current_text = []\n",
    "            \n",
    "            for part in parts:\n",
    "                # If we see a P or NP, it's likely the end of a column\n",
    "                if part in ['P', 'NP', 'SCN']:\n",
    "                    current_text.append(part)\n",
    "                    columns[current_col] = ' '.join(current_text)\n",
    "                    current_col += 1\n",
    "                    current_text = []\n",
    "                else:\n",
    "                    current_text.append(part)\n",
    "            \n",
    "            # Add any remaining text to the last column\n",
    "            if current_text:\n",
    "                columns[current_col] = ' '.join(current_text)\n",
    "            \n",
    "            # Add the category if we have one\n",
    "            if current_category:\n",
    "                columns.insert(0, current_category)\n",
    "            \n",
    "            # Add the row to our data\n",
    "            data.append(columns)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Clean up the DataFrame\n",
    "        # Remove rows where all columns are empty\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        # Forward fill category values\n",
    "        df[0] = df[0].fillna(method='ffill')\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Extract lines\n",
    "pdf_path = 'WI.pdf'\n",
    "df = extract_lines_from_pdf(pdf_path)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = 'page_2_tables.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"First few rows of extracted data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[25], line 76\u001b[0m\n",
      "\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Extract lines\u001b[39;00m\n",
      "\u001b[1;32m     75\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWI.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m---> 76\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_lines_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n",
      "\u001b[1;32m     79\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_2_tables.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "Cell \u001b[0;32mIn[25], line 45\u001b[0m, in \u001b[0;36mextract_lines_from_pdf\u001b[0;34m(pdf_path, page_number)\u001b[0m\n",
      "\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSCN\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[1;32m     44\u001b[0m     current_text\u001b[38;5;241m.\u001b[39mappend(part)\n",
      "\u001b[0;32m---> 45\u001b[0m     \u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_col\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(current_text)\n",
      "\u001b[1;32m     46\u001b[0m     current_col \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;32m     47\u001b[0m     current_text \u001b[38;5;241m=\u001b[39m []\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "def extract_lines_from_pdf(pdf_path, page_number=2):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        page = pdf.pages[page_number-1]\n",
    "        \n",
    "        # Extract text line by line\n",
    "        text = page.extract_text()\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        # Process each line\n",
    "        data = []\n",
    "        current_category = None\n",
    "        \n",
    "        for line in lines:\n",
    "            # Skip header and footer lines\n",
    "            if 'Page' in line or 'Uses PA/PDL' in line or 'Exemption Form' in line:\n",
    "                continue\n",
    "                \n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # Check if this is a category line (all caps)\n",
    "            if line.isupper() and len(line) > 3:\n",
    "                current_category = line\n",
    "                continue\n",
    "            \n",
    "            # Split the line into columns (approximately)\n",
    "            # The line should have 4 columns separated by spaces\n",
    "            parts = line.split()\n",
    "            \n",
    "            # Initialize columns\n",
    "            columns = [''] * 4\n",
    "            \n",
    "            # Try to distribute the parts into columns\n",
    "            current_col = 0\n",
    "            current_text = []\n",
    "            \n",
    "            for part in parts:\n",
    "                # If we see a P or NP, it's likely the end of a column\n",
    "                if part in ['P', 'NP', 'SCN']:\n",
    "                    current_text.append(part)\n",
    "                    columns[current_col] = ' '.join(current_text)\n",
    "                    current_col += 1\n",
    "                    current_text = []\n",
    "                else:\n",
    "                    current_text.append(part)\n",
    "            \n",
    "            # Add any remaining text to the last column\n",
    "            if current_text:\n",
    "                columns[current_col] = ' '.join(current_text)\n",
    "            \n",
    "            # Add the category if we have one\n",
    "            if current_category:\n",
    "                columns.insert(0, current_category)\n",
    "            \n",
    "            # Add the row to our data\n",
    "            data.append(columns)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Clean up the DataFrame\n",
    "        # Remove rows where all columns are empty\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        # Forward fill category values\n",
    "        df[0] = df[0].fillna(method='ffill')\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Extract lines\n",
    "pdf_path = 'WI.pdf'\n",
    "df = extract_lines_from_pdf(pdf_path)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = 'page_2_tables.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"First few rows of extracted data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[25], line 76\u001b[0m\n",
      "\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Extract lines\u001b[39;00m\n",
      "\u001b[1;32m     75\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWI.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m---> 76\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mextract_lines_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n",
      "\u001b[1;32m     79\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_2_tables.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\n",
      "Cell \u001b[0;32mIn[25], line 45\u001b[0m, in \u001b[0;36mextract_lines_from_pdf\u001b[0;34m(pdf_path, page_number)\u001b[0m\n",
      "\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSCN\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[1;32m     44\u001b[0m     current_text\u001b[38;5;241m.\u001b[39mappend(part)\n",
      "\u001b[0;32m---> 45\u001b[0m     \u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_col\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(current_text)\n",
      "\u001b[1;32m     46\u001b[0m     current_col \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;32m     47\u001b[0m     current_text \u001b[38;5;241m=\u001b[39m []\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "def extract_lines_from_pdf(pdf_path, page_number=2):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        page = pdf.pages[page_number-1]\n",
    "        \n",
    "        # Extract text line by line\n",
    "        text = page.extract_text()\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        # Process each line\n",
    "        data = []\n",
    "        current_category = None\n",
    "        \n",
    "        for line in lines:\n",
    "            # Skip header and footer lines\n",
    "            if 'Page' in line or 'Uses PA/PDL' in line or 'Exemption Form' in line:\n",
    "                continue\n",
    "                \n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # Check if this is a category line (all caps)\n",
    "            if line.isupper() and len(line) > 3:\n",
    "                current_category = line\n",
    "                continue\n",
    "            \n",
    "            # Split the line into columns (approximately)\n",
    "            # The line should have 4 columns separated by spaces\n",
    "            parts = line.split()\n",
    "            \n",
    "            # Initialize columns\n",
    "            columns = [''] * 4\n",
    "            \n",
    "            # Try to distribute the parts into columns\n",
    "            current_col = 0\n",
    "            current_text = []\n",
    "            \n",
    "            for part in parts:\n",
    "                # If we see a P or NP, it's likely the end of a column\n",
    "                if part in ['P', 'NP', 'SCN']:\n",
    "                    current_text.append(part)\n",
    "                    columns[current_col] = ' '.join(current_text)\n",
    "                    current_col += 1\n",
    "                    current_text = []\n",
    "                else:\n",
    "                    current_text.append(part)\n",
    "            \n",
    "            # Add any remaining text to the last column\n",
    "            if current_text:\n",
    "                columns[current_col] = ' '.join(current_text)\n",
    "            \n",
    "            # Add the category if we have one\n",
    "            if current_category:\n",
    "                columns.insert(0, current_category)\n",
    "            \n",
    "            # Add the row to our data\n",
    "            data.append(columns)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Clean up the DataFrame\n",
    "        # Remove rows where all columns are empty\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        # Forward fill category values\n",
    "        df[0] = df[0].fillna(method='ffill')\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Extract lines\n",
    "pdf_path = 'WI.pdf'\n",
    "df = extract_lines_from_pdf(pdf_path)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = 'page_2_tables.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"First few rows of extracted data:\")\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
